task_name: train
tags:
- H8Cloude
- Convlstm
train: true
test: true
compile: false
ckpt_path: null
seed: 42
data:
  _target_: src.data.H8Cloud_datamodule.H8CLoudDataModule
  train_config:
    data_dir: ${paths.data_dir}/DLDATA/H8JPEG_valid
    use_transform: true
    get_optical_flow: null
    split_path: null
    step: 1
    Intinterval: 19
    crop_size:
    - 256
    - 256
  val_config:
    data_dir: ${paths.data_dir}/validate_croped
    use_transform: false
    get_optical_flow: null
    split_path: null
    step: 1
    Intinterval: 19
    crop_size:
    - 256
    - 256
  test_config:
    data_dir: ${paths.data_dir}/validate_croped
    use_transform: false
    get_optical_flow: null
    split_path: ${paths.data_dir}/valid.json
    step: 19
    Intinterval: 19
  batch_size: 4
  batch_size_val: 4
  num_workers: 8
  pin_memory: true
model:
  _target_: src.models.rnn.RNNModule
  net:
    _target_: src.models.components.conv_lstm.ConvLSTM_Model
    num_layers: 4
    num_hidden:
    - 128
    - 128
    - 128
    - 128
    configs:
      in_shape:
      - 19
      - 3
      - 256
      - 256
      patch_size: 2
      pre_seq_length: 7
      aft_seq_length: 12
      total_length: 19
      filter_size: 5
      stride: 1
      layer_norm: 0
      scheduled_sampling: 1
      sampling_stop_iter: ${sampling_stop_iter}
      sampling_start_value: 1.0
      sampling_changing_rate: 2.0e-05
  loss:
    _target_: torch.nn.MSELoss
  hparams:
    TRAIN:
      EPOCHS: ${max_iters}
      INTERVAL: ${OptimizerInterval}
      MONITOR: ${monitor}
      WARMUP_EPOCHS: 0
      BASE_LR: 0.0003
      T_IN_EPOCHS: true
      WARMUP_LR: 1.0e-07
      MIN_LR: 1.0e-07
      WEIGHT_DECAY: 0.001
      OPTIMIZER:
        NAME: adamw
        MOMENTUM: 0.9
        EPS: 1.0e-08
        BETAS:
        - 0.9
        - 0.999
      LR_SCHEDULER:
        NAME: linear
        DECAY_RATE: 0.1
        DECAY_EPOCHS: 6
        MODE: max
        PATIENCE: 10
        GAMMA: 0.9
callbacks:
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${paths.output_dir}/checkpoints
    filename: step_{step:03d}_loss{val/loss:.3f}
    monitor: ${monitor}
    verbose: false
    save_last: true
    save_top_k: 2
    mode: min
    auto_insert_metric_name: false
    save_weights_only: false
    every_n_train_steps: ${ckpt_interval}
    train_time_interval: null
    every_n_epochs: null
    save_on_train_epoch_end: null
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: ${monitor}
    min_delta: 0.0
    patience: 100
    verbose: false
    mode: min
    strict: true
    check_finite: true
    stopping_threshold: null
    divergence_threshold: null
    check_on_train_epoch_end: null
  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: -1
  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar
  learning_rate_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: ${OptimizerInterval}
logger:
  wandb:
    _target_: lightning.pytorch.loggers.wandb.WandbLogger
    save_dir: ${paths.output_dir}
    offline: false
    id: null
    anonymous: null
    project: lightning-hydra-template
    log_model: false
    prefix: ''
    group: ''
    tags: []
    job_type: ''
trainer:
  _target_: lightning.pytorch.trainer.Trainer
  default_root_dir: ${paths.output_dir}
  min_epochs: 1
  max_epochs: -1
  accelerator: gpu
  devices: 1
  check_val_every_n_epoch: null
  deterministic: false
  max_steps: ${max_iters}
  val_check_interval: ${interval}
  log_every_n_steps: ${log_interval}
  precision: 16
  gradient_clip_val: 0.5
  gradient_clip_algorithm: norm
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  data_dir: ${paths.root_dir}/data/
  log_dir: ${paths.root_dir}/logs/
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}
extras:
  ignore_warnings: false
  enforce_tags: true
  print_config: true
OptimizerInterval: step
max_iters: 16000
sampling_stop_iter: 13000
interval: 1600
ckpt_interval: 1605
log_interval: 160
monitor: val/loss
